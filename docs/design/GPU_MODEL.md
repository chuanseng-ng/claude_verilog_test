# GPU Execution Model

**Feasible by design**

## Design goals

‚ùå No:

- Cache coherence
- Preemption
- Out-of-order
- Dynamic scheduling

‚úÖ Yes:

- SIMT-like execution
- Deterministic behavior
- CPU-managed launches

## Execution model (Simple but real)

Key concepts

- Grid ‚Üí Blocks ‚Üí Warps
- Fixed warp size (e.g. 8 or 16 lanes)
- Single instruction per warp

Control

- One Warp Scheduler
- Round-robin warp issue
- No scoreboarding across warps

## Pipeline (GPU core)

```text
Fetch ‚Üí Decode ‚Üí Execute ‚Üí Memory ‚Üí Writeback
```

- Shared instruction per wrap
- Per-lane register files
- Mask register for divergence

## Divergence handling

```text
if (cond) {
  A;
} else {
  B;
}
```

Implemented via:

- Per-lane predicate mask
- Serial execution of paths
- Mask restore

üö´ No reconvergence stacks beyond 1 level initially

## Memory model

- Global memory only
- Coalesced access when lanes hit same cache line
- Otherwise serialized

## AI vs Human scope

| Component     | AI  | Human  |
| :-----------: | :-: | :----: |
| Vector ALU    | ‚úÖ  | Review |
| Warp FSM      | ‚ö†Ô∏è  | ‚úÖ     |
| Divergence    | ‚ùå  | ‚úÖ     |
| Kernel launch | ‚ö†Ô∏è  | ‚úÖ     |
| Test kernels  | ‚úÖ  | Review |

## GPU verification (Python)

Reference kernel execution

```python
def execute_kernel(kernel, data):
    for warp in warps:
        for pc in kernel:
            execute_instruction(pc, warp)
```

Scoreboard compares:

- Memory outputs
- Per-lane registers
